{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19c41f88",
      "metadata": {
        "id": "19c41f88"
      },
      "source": [
        "# Homework: Documenting Your Code + Testing Your Code\n",
        "\n",
        "## Problem 1 - Write docstrings\n",
        "\n",
        "The following functions are missing docstrings. Write Google-style docstrings for each function, including `Args`, `Returns`, and `Raises` sections where appropriate. Make sure to document default values and explain what each parameter means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cacdf2c",
      "metadata": {
        "id": "2cacdf2c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def normalize(data, method=\"zscore\"):\n",
        "    \"\"\"Normalize the input data using the specified method.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Input data to normalize.\n",
        "        method (str): Normalization method, either \"zscore\" (normalizes to mean 0 and std 1) \n",
        "                      or \"minmax\" (scales data to the range [0, 1]). Defaults to \"zscore\". \n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray) Normalized data.\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: If method is not \"zscore\" or \"minmax\".\n",
        "    \"\"\"\n",
        "    if method == \"zscore\":\n",
        "        return (data - np.mean(data)) / np.std(data)\n",
        "    elif method == \"minmax\":\n",
        "        return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "\n",
        "def weighted_mean(values, weights=None):\n",
        "    \"\"\"Calculate the weighted mean.\n",
        "    \n",
        "    Args:\n",
        "        values (np.ndarray): Array of values.\n",
        "        weights (np.ndarray or None): Array of weights with the same length as values.\n",
        "                                    If None, calculates the unweighted mean. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        (float) Weighted mean of the values.\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: If weights is not None and its length does not match values.\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        return np.mean(values)\n",
        "    if len(values) != len(weights):\n",
        "        raise ValueError(\"values and weights must have the same length\")\n",
        "    return np.sum(values * weights) / np.sum(weights)\n",
        "\n",
        "\n",
        "def remove_outliers(data, threshold=3.0):\n",
        "    \"\"\"Remove outliers from the data based on a z-score threshold.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): Input data from which to remove outliers.\n",
        "        threshold (float): Z-score threshold to identify outliers. Defaults to 3.0.\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray) Data with outliers removed.\n",
        "    \"\"\"\n",
        "    mean = np.mean(data)\n",
        "    std = np.std(data)\n",
        "    mask = np.abs(data - mean) <= threshold * std\n",
        "    return data[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f16fb210",
      "metadata": {
        "id": "f16fb210"
      },
      "source": [
        "## Problem 2 - Add type hints\n",
        "\n",
        "The following functions have incomplete or missing type hints. Add appropriate type hints for all parameters and return values. Use `|` syntax for union types where a parameter can accept multiple types or return `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e458a748",
      "metadata": {
        "id": "e458a748"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def clip_values(arr: np.ndarray, lower: float, upper: float) -> np.ndarray:\n",
        "    \"\"\"Clip array values to be within [lower, upper] range.\"\"\"\n",
        "    return np.clip(arr, lower, upper)\n",
        "\n",
        "\n",
        "def find_peaks(data: np.ndarray | list, min_height: float | None = None) -> list | None:\n",
        "    \"\"\"Find indices where values are local maxima above min_height.\n",
        "\n",
        "    Returns None if no peaks are found.\n",
        "    \"\"\"\n",
        "    peaks = []\n",
        "    for i in range(1, len(data) - 1):\n",
        "        if data[i] > data[i - 1] and data[i] > data[i + 1]:\n",
        "            if min_height is None or data[i] >= min_height:\n",
        "                peaks.append(i)\n",
        "    if len(peaks) == 0:\n",
        "        return None\n",
        "    return peaks\n",
        "\n",
        "\n",
        "def summarize(data: np.ndarray, stats: list) -> dict:\n",
        "    \"\"\"Calculate summary statistics for data.\n",
        "\n",
        "    Args:\n",
        "        data: Input array of numeric values.\n",
        "        stats: List of statistic names to compute.\n",
        "            Valid options: \"mean\", \"median\", \"std\", \"min\", \"max\"\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping statistic names to computed values.\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    for stat in stats:\n",
        "        if stat == \"mean\": result[stat] = np.mean(data)\n",
        "        elif stat == \"median\":\n",
        "            result[stat] = np.median(data)\n",
        "        elif stat == \"std\":\n",
        "            result[stat] = np.std(data)\n",
        "        elif stat == \"min\":\n",
        "            result[stat] = np.min(data)\n",
        "        elif stat == \"max\":\n",
        "            result[stat] = np.max(data)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b66a6400",
      "metadata": {
        "id": "b66a6400"
      },
      "source": [
        "## Problem 3: Identifying Test Types\n",
        "\n",
        "For each scenario below, identify whether the test being described is a **unit test**, **integration test**, or **regression test**. Briefly explain your reasoning.\n",
        "\n",
        "**(a)** You write a test that verifies `calculate_variance()` returns 0 for the input `[3.0, 3.0, 3.0]`.\n",
        "\n",
        "This is a unit test, since it verifies that an individual function works correctly in isolation.\n",
        "\n",
        "**(b)** After discovering that `fit_model()` crashes when given a dataset with a single row, you fix the bug and add a test with a one-row input.\n",
        "\n",
        "This is a regression test, as it verifies that previously fixed bugs don't reappear.\n",
        "\n",
        "**(c)** You write a test that loads data from a CSV file, passes it through `clean_data()`, fits a model with `fit_linear_regression()`, and verifies the model's R-squared value is within an expected range.\n",
        "\n",
        "This is an integration test, as it verifies that multiple components work together correctly.\n",
        "\n",
        "**(d)** A user reports that `normalize()` returns incorrect values when all input values are negative. After fixing the issue, you add a test with input `[-5.0, -3.0, -1.0]`.\n",
        "\n",
        "This is a regression test, as it verifies that previously fixed bugs don't reappear.\n",
        "\n",
        "## Problem 4: Code Review - What's Wrong with These Tests?\n",
        "\n",
        "Review the following test code and identify at least **four** problems with the test design or implementation. Explain why each is problematic and suggest how to fix it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0f31e7",
      "metadata": {
        "id": "bb0f31e7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#--------------------------------\n",
        "# The following test function tests multiple things at one time.\n",
        "# It should be split into separate test functions, each testing one aspect.\n",
        "def test_all_statistics():\n",
        "    data = [10, 20, 30, 40, 50]\n",
        "\n",
        "    # Test mean\n",
        "    assert np.mean(data) == 30\n",
        "\n",
        "    # Test median\n",
        "    assert np.median(data) == 30\n",
        "\n",
        "    # Test standard deviation\n",
        "    assert np.std(data) > 0\n",
        "\n",
        "    # Test min and max\n",
        "    assert np.min(data) == 10\n",
        "    assert np.max(data) == 50\n",
        "\n",
        "    # Test sum\n",
        "    assert np.sum(data) == 150\n",
        "\n",
        "#--------------------------------\n",
        "# The following test function needs to be renamed.\n",
        "# A good pattern is `test_<function>_<scenario>_<expected_result>`\n",
        "# For example, the function name could be `test_npvar_returns_non_negative`\n",
        "def verify_variance_positive(arr):\n",
        "    var = np.var(arr)\n",
        "    assert var >= 0\n",
        "\n",
        "#--------------------------------\n",
        "# The following test function doesn't take into account floating point precision.\n",
        "# It should use `math.isclose` or other ways to compare with some tolerance.\n",
        "def test_correlation():\n",
        "    x = np.array([1.0, 2.0, 3.0])\n",
        "    y = np.array([2.0, 4.0, 6.0])\n",
        "    corr = np.corrcoef(x, y)[0, 1]\n",
        "    assert corr == 1.0\n",
        "\n",
        "#--------------------------------\n",
        "# The two test functions below should be combined into one, and we should create a\n",
        "# `results` list inside the function. Otherwise, the two tests will depend on each other.\n",
        "results = []\n",
        "\n",
        "def test_append_result():\n",
        "    global results\n",
        "    results.append(42)\n",
        "    assert 42 in results\n",
        "\n",
        "def test_check_results():\n",
        "    assert len(results) == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3030fab2",
      "metadata": {
        "id": "3030fab2"
      },
      "source": [
        "## Problem 5: The Flaky Test\n",
        "\n",
        "Your colleague wrote the following test for a bootstrap confidence interval function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "235ca02a",
      "metadata": {
        "id": "235ca02a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def bootstrap_ci(data, confidence=0.95, n_bootstrap=1000):\n",
        "    \"\"\"Compute bootstrap confidence interval for the mean.\"\"\"\n",
        "    means = []\n",
        "    n = len(data)\n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(data, size=n, replace=True)\n",
        "        means.append(np.mean(sample))\n",
        "\n",
        "    alpha = 1 - confidence\n",
        "    lower = np.percentile(means, 100 * alpha / 2)\n",
        "    upper = np.percentile(means, 100 * (1 - alpha / 2))\n",
        "    return lower, upper\n",
        "\n",
        "def test_bootstrap_ci_contains_true_mean():\n",
        "    data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "    true_mean = 5.5\n",
        "    lower, upper = bootstrap_ci(data)\n",
        "    assert lower < true_mean < upper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "860f28d3",
      "metadata": {
        "id": "860f28d3"
      },
      "source": [
        "**(a)** The test passes most of the time but occasionally fails. Explain why this test is \"flaky\" (non-deterministic).\n",
        "\n",
        "A random seed should have been set. Using random state will also work.\n",
        "\n",
        "**(b)** Your colleague argues: \"The test is correct because a 95% confidence interval should contain the true mean 95% of the time, so occasional failures are expected.\" Is this a good argument for keeping the test as-is? Why or why not?\n",
        "\n",
        "The function to be tested involves randomness. Testing such code requires careful handling to get reproducible results. A \"flaky\" test isn't good.\n",
        "\n",
        "**(c)** Rewrite the test to be deterministic and reliable while still meaningfully testing the `bootstrap_ci` function. Your solution should: ensure reproducible results and verify that the confidence interval has reasonable properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5a8a8515",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_bootstrap_ci_contains_true_mean_v2():\n",
        "    np.random.seed(0)\n",
        "    data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "    true_mean = 5.5\n",
        "    lower, upper = bootstrap_ci(data)\n",
        "    assert lower < true_mean < upper\n",
        "\n",
        "test_bootstrap_ci_contains_true_mean_v2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce558899",
      "metadata": {},
      "source": [
        "\n",
        "**(d)** Propose an alternative testing strategy that could verify the 95% coverage property without making the test flaky. You don't need to implement it, but describe the approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc5c837",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_bootstrap_ci_contains_true_mean_v3():\n",
        "    np.random.seed(0)\n",
        "    data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "    true_mean = 5.5\n",
        "    \n",
        "    n_trials = 200\n",
        "    covered_count = 0\n",
        "\n",
        "    for i in range(n_trials):\n",
        "        lower, upper = bootstrap_ci(data)\n",
        "        if lower < true_mean < upper:\n",
        "            covered_count += 1\n",
        "\n",
        "    coverage = covered_count / n_trials\n",
        "    assert coverage >= 0.90\n",
        "\n",
        "test_bootstrap_ci_contains_true_mean_v3()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
